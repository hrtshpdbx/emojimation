{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vartikatrao/Emojimation/blob/Tkinter_GUI/emojimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvTdfJMSIN5x"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import load_img\n",
        "from keras.layers import Conv2D, Dense, BatchNormalization, Activation, Dropout, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "import datetime\n",
        "from keras import regularizers\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRry6Wa7xt6n"
      },
      "outputs": [],
      "source": [
        "# Configure kaggle\n",
        "os.chdir('/root/')\n",
        "!mkdir -p .kaggle\n",
        "os.chdir('/root/.kaggle')\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Y-o0TVcjehM8SZB3Nt8U3xkyeQu-Nse-' -O kaggle.json > /dev/null 2>&1\n",
        "!ls /root/.kaggle\n",
        "\n",
        "# Set permissions\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# Create data folder\n",
        "os.chdir('/content/')\n",
        "!rm -rf data\n",
        "!mkdir data\n",
        "os.chdir('data')\n",
        "!pwd\n",
        "\n",
        "# Download data\n",
        "!pip install -q kaggle\n",
        "!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge\n",
        "!pwd\n",
        "!ls\n",
        "# Unzip data\n",
        "!unzip challenges-in-representation-learning-facial-expression-recognition-challenge.zip train.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkp8yOsfUdVr"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('train.csv')\n",
        "print(data.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFfJQbLrGLFd"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy\n",
        "\n",
        "train_images = []\n",
        "train_labels = []\n",
        "\n",
        "categories_count = {}\n",
        "\n",
        "with open('train.csv') as train:\n",
        "\n",
        "    # Read train.csv file\n",
        "    csv_reader = csv.reader(train)\n",
        "\n",
        "    next(csv_reader)  # Skip the header\n",
        "\n",
        "    for row in csv_reader:\n",
        "\n",
        "        # Append image\n",
        "        pixels_str = row[1]\n",
        "        pixels_list = [int(i) for i in pixels_str.split(' ')]\n",
        "        pixels_list = numpy.array(pixels_list, dtype='uint8')\n",
        "        image = pixels_list.reshape((48, 48))\n",
        "        train_images.append(image)\n",
        "\n",
        "        label_str = row[0]\n",
        "\n",
        "        # Calculate categories count\n",
        "        count = 0\n",
        "        if label_str in categories_count:\n",
        "            count = categories_count[label_str] + 1\n",
        "        categories_count[label_str] = count\n",
        "\n",
        "        # Append label\n",
        "        label = int(label_str)\n",
        "        train_labels.append(label)\n",
        "\n",
        "# Create numpy array of train images and labels\n",
        "x_train = numpy.array(train_images)\n",
        "y_train = numpy.array(train_labels)\n",
        "\n",
        "print('x_train shape: {0}'.format(x_train.shape))\n",
        "print('y_train shape: {0}'.format(y_train.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E78lmkmTIMLm"
      },
      "outputs": [],
      "source": [
        "categories = ('Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral')\n",
        "y_pos = numpy.arange(len(categories))\n",
        "\n",
        "counts = []\n",
        "for label in range(len(categories)):\n",
        "    label_str = str(label)\n",
        "    count = categories_count[label_str]\n",
        "    counts.append(count)\n",
        "\n",
        "# Draw histogram\n",
        "plt.bar(y_pos, counts, align='center')\n",
        "plt.xticks(y_pos, categories)\n",
        "plt.ylabel('Count')\n",
        "plt.title('FER2013 Dataset Categories')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BByy7-yVAV_"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Split dataset into train set and test set\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2)\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "x_train = x_train.reshape(len(x_train), 48, 48, 1)\n",
        "x_test = x_test.reshape(len(x_test), 48, 48, 1)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yBLXentEEFf"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print('Tensorflow version: {}'.format(tf.__version__))\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPool2D, Dropout, Flatten, Dense\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from tensorflow.keras import Model, Input\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgXpG6CnEfVI"
      },
      "outputs": [],
      "source": [
        "cnn_model = Sequential()\n",
        "\n",
        "# 1st convolution layer\n",
        "cnn_model.add(Conv2D(64, input_shape=(48, 48, 1), kernel_size=(3, 3), activation='relu'))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(Conv2D(64, padding='same', kernel_size=(3, 3), activation='relu'))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(MaxPool2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "cnn_model.add(Dropout(0.3))\n",
        "\n",
        "# 2nd convolution layer\n",
        "cnn_model.add(Conv2D(128, padding='same', kernel_size=(3, 3), activation='relu'))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(Conv2D(128, padding='same', kernel_size=(3, 3), activation='relu'))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(MaxPool2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "cnn_model.add(Dropout(0.3))\n",
        "\n",
        "# 3rd convolution layer\n",
        "cnn_model.add(Conv2D(256, padding='same', kernel_size=(3, 3), activation='relu'))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(Conv2D(256, padding='same', kernel_size=(3, 3), activation='relu'))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(MaxPool2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "cnn_model.add(Dropout(0.3))\n",
        "\n",
        "# 4th convolution layer\n",
        "cnn_model.add(Conv2D(512, padding='same', kernel_size=(3, 3), activation='relu'))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(Conv2D(512, padding='same', kernel_size=(3, 3), activation='relu'))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(MaxPool2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "cnn_model.add(Dropout(0.3))\n",
        "\n",
        "# Fully connected layer\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(512, activation='relu'))\n",
        "cnn_model.add(Dropout(0.3))\n",
        "cnn_model.add(Dense(256, activation='relu'))\n",
        "cnn_model.add(Dropout(0.3))\n",
        "cnn_model.add(Dense(64, activation='relu'))\n",
        "cnn_model.add(Dropout(0.3))\n",
        "\n",
        "cnn_model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "adam = Adam(learning_rate=0.001)\n",
        "cnn_model.compile(optimizer=adam,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Summary the model\n",
        "cnn_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XHrH736Erfb"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10)\n",
        "reduce_learning_rate = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=5)\n",
        "\n",
        "# Train the model\n",
        "history = cnn_model.fit(x_train,\n",
        "                        y_train,\n",
        "                        batch_size=64,\n",
        "                        epochs=100,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        callbacks=[early_stopping, reduce_learning_rate])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRpdMFDPlfac"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model before improvement\n",
        "_, score_before_improvement = cnn_model.evaluate(x_test, y_test)\n",
        "print('Score before improvement: {}'.format(score_before_improvement))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QITj1fHsl64L"
      },
      "outputs": [],
      "source": [
        "# Generate hard data- data the model wrongly classified\n",
        "hard_images = []\n",
        "hard_labels = []\n",
        "\n",
        "# Make predictions\n",
        "predictions = cnn_model.predict(x_test)\n",
        "for i, v in enumerate(predictions):\n",
        "    y_predict = numpy.argmax(v)\n",
        "    y_real = y_train[i]\n",
        "    if y_predict != y_real:\n",
        "        # If predict incorrectly, append to array\n",
        "        image = x_train[i]\n",
        "        hard_image = image.reshape(1, 48, 48, 1)\n",
        "        hard_images.append(image)\n",
        "        hard_labels.append(y_real)\n",
        "\n",
        "x_hard = numpy.array(hard_images)\n",
        "y_hard = numpy.array(hard_labels)\n",
        "\n",
        "print(x_hard.shape)\n",
        "print(y_hard.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELf8AdcomAih"
      },
      "outputs": [],
      "source": [
        "# Train the model on hard data\n",
        "x_hard_train, x_hard_test, y_hard_train, y_hard_test = train_test_split(x_hard, y_hard, test_size=0.2)\n",
        "history = cnn_model.fit(x_hard_train,\n",
        "                        y_hard_train,\n",
        "                        batch_size=64,\n",
        "                        epochs=100,\n",
        "                        validation_data=(x_hard_test, y_hard_test),\n",
        "                        callbacks=[early_stopping, reduce_learning_rate])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vc-tNAtOmQky"
      },
      "outputs": [],
      "source": [
        "# Perform data augmentation\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "data_generator = ImageDataGenerator(featurewise_center=False,\n",
        "                                    featurewise_std_normalization=False,\n",
        "                                    rotation_range=10,\n",
        "                                    width_shift_range=0.1,\n",
        "                                    height_shift_range=0.1,\n",
        "                                    zoom_range=.1,\n",
        "                                    horizontal_flip=True)\n",
        "flow = data_generator.flow(x_train,\n",
        "                           y_train,\n",
        "                           batch_size=64)\n",
        "\n",
        "# Train the model again to balance out\n",
        "history = cnn_model.fit(flow,\n",
        "                        epochs=100,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        callbacks=[early_stopping, reduce_learning_rate])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNKKtItqHpnb"
      },
      "outputs": [],
      "source": [
        "!pip install -U mlxtend > /dev/null 2>&1\n",
        "\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Make predictions\n",
        "predictions = cnn_model.predict(x_test)\n",
        "y_predict = numpy.argmax(predictions, axis=1)\n",
        "\n",
        "# Create confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_predict)\n",
        "\n",
        "# Display confusion matrix\n",
        "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "fig, ax = plot_confusion_matrix(conf_mat=conf_matrix, class_names=class_names)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUJIHW5UHqAz"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/')\n",
        "!rm -rf build\n",
        "!mkdir build\n",
        "os.chdir('build')\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.save('FER2013.h5')"
      ],
      "metadata": {
        "id": "N0iTQke2AqCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tensorflow lite model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(cnn_model)\n",
        "tflite_model = converter.convert()\n",
        "open(\"FER2013.tflite\", \"wb\").write(tflite_model)"
      ],
      "metadata": {
        "id": "6q1Ubi6wBgvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"model.h5\")"
      ],
      "metadata": {
        "id": "4cQM-hLMaUUg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1ODboesNSGA-UH2JIKTduHUay1bwGb6ef",
      "authorship_tag": "ABX9TyNzZwhX0MZJaLbmrn8SgbC4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}